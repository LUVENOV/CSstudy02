## 3.2 메모리

CPU는 그저 '메모리'에 올라와있는 프로그램의 명령어를 실행할 뿐임!

### 3.2.1 메모리 계층

- 저장장치 계층 구조
  - Resister
  - Cache (L1, L2, L3)
  - (Main) Memoty (RAM : Random Access Memory)
  - 2nd Memory : Magnetic Disk > Oprical Disk > Magentic Tape

* 계층 위로 올라갈 수록 가격 상승, 용량 하락, 속도 증가

#### 캐시

: 데이터를 미리 복사해놓는 임시 저장소

- 목적 : 데이터 접근 속도 차이에 따른 병목현상을 줄이기 위함

- 캐싱 계층

  - 속도 차이를 해결하기 위해 계층과 계층 사이 있는 메모리 계층
  - 예시 : 캐시 메모리와 보조기억 장치 사이 주기억장치를 캐싱 계층이라 할 수 있음

- 지역성 원리
  캐시 데이터 설정에 대한 근거 : 어떤 데이터를 자주 사용하는가?
  - 시간 지역성 : 최근 사용한 데이터에 다시 접근하려는 특징
    - 예시 : for문의 변수 i를 다시 접근
  - 공간 지역성 : 최근 접근한 데이터를 이루고 있는 공간에 가까운 공간에 접근하려는 특성
    - 예시 : 배열 arr의 각 요소 접근

#### 캐시히트 & 캐시미스

- 캐시 히트 : 캐시에서 원하는 데이터를 찾은 것
  - CPU 내부 버스 기반으로 작동하여 빠름
- 캐시 미스 : 캐시에서 원하는 데이터가 없어 주 메모리에서 데이터를 찾아오는 것

  - 시스템 버스 기반으로 작동하여 느림

- **캐시 매핑**

  - 캐시가 히트되기 위해 매핑
  - Block : 지역성 원리에 의해 데이터를 한번 퍼낼 때의 최소 단위
  - CPU의 레지스터와 주 메모리 간 데이터 주고 받을 때를 예시로 설명
    1. **직접 매핑**
       - 순서대로 캐시와 메모리 매핑
       - 속도가 빠르지만 충돌이 잦음
       - 예시 : 메모리가 1~100까지 있고 캐시가 1~10까지 있다면 1~10까지의 메모리는 캐시의 1에 위치하고 11~20까지의 메모리는 캐시의 2에 위치
       - 단점 예시 : 30~40에 해당하는 값을 자꾸 불러다 사용해야 하는데 이를 저장할 캐시 공간은 3 하나 뿐이므로 매번 캐시 교체가 일어나게 된다
    2. **연관 매핑**
       - 순서 일치시키지 않고 관련있는 캐시와 메모리 매핑
       - 충돌이 적으나 모든 블록 탐색 필요하여 속도가 느림
    3. **집합 연관 매핑**
       - 직접 매핑과 연관 매핑 합침
       - 순서는 일치시키나 집합을 둬서 보다 효울적으로 검색 가능
       - 예시 : 메모리가 1~100까지 있고 캐시가 1~10까지 있다면 캐시 1~5에는 1~50의 데이터를 무작위로 저장
  - [참조](https://ssoonidev.tistory.com/35)

- **웹 브라우저의 캐시**

  - 서버 요청할 때 클라이언트 아이덴티티 확인 or 중복 요청 방지를 위해 쓰임
  - **쿠키 : 만료기한이 있는 키 - 값 저장소**

    - 최대 4KB
    - 웹브라우저에 저장되며 웹 브라우저는 향후 사용자가 웹 서버에 요청할 때 관련 쿠키를 첨부
    - 기능
      - 사용자 세션 : 웹 사이트 활동을 특정 사용자와 연결
      - 개인화 : 웹 사이트가 사용자 경험을 맞춤화하는 데 사용
      - 인증 쿠키(아래 참조)와 같이 보안을 위해 사용
    - 그러나 일부 쿠키에는 사용자의 웹 사이트 이용 내역이 기록되어 개인 정보 문제 발생 가능
    - 보통 서버쪽에서 만료기한을 정함
    - 쿠키 설정시 document.cookie로 쿠키를 볼 수 없게 httponly 옵션을 거는 것이 중요

  - **로컬 스토리지 : 만료기한이 없는 키-값 저장소**

    - 최대 10MB
    - **도메인 단위로 저장, 생성 되며 웹 브라우저를 닫아도 유지**
    - HTML5 지원해야 사용 가능
    - 서버에서 수정 불가능 (클라이언트만 수정 가능)

  - **세션 스토리지 : 만료기한이 없는 키-값 저장소**
    - 최대 5MB
    - **탭 단위로 생성, 삭제 되며 브라우저 탭을 닫으면 제거됨**
    - HTML5 지원해야 사용 가능
    - 서버에서 수정 불가능 (클라이언트만 수정 가능)

[참조] (https://www.zerocho.com/category/HTML&DOM/post/5918515b1ed39f00182d3048)

- 데이터베이스에서의 캐싱 계층
  - 메인 데이터베이스 위 redis 데이터베이스 계층을 캐싱 계층으로 두어 성능 향상하기도

### 3.2.2 메모리 관리

: OS의 대표적인 업무

#### 가상 메모리

- 메모리 관리 기법의 하나로 하나의 컴퓨터가 실제로 이용가능한 메모리자원을 추상화하여 이를 사용하는 사용자들에게 메우 큰 메모리로 보이게 만드는 것

- 가상 주소 (논리 주소): CPU가 보는 주소로 프로세스마다 독립적으로 가지며 각각 0번지부터 보는 주소
  - MMU (HW)에 의해 실제 주소로 변환
- 실제 주소 (물리 주소): 실제 메모리 상에 있는 주소

* 사용자는 물리주소를 알 수 없고 알 필요도 없다

* 가상 메모리 : 가상 주소와 실제 주소 매핑 (주소 바인딩)
  - 프로세스 주소 정보가 들어있는 "페이지 테이블"로 관리
  - TLB : 메모리와 CPU 사이 주소 변환을 위한 캐시 (페이징 항목에서 더 자세히 다루겠다)
    - 페이지 테이블에 있는 리스트를 보관
    - CPU 가 페이지 테이블까지 가지 않도록 하여 속도를 향상시키는 캐시 계층
  * 언제 주소가 바인딩되는가?
    - Compile Time Binding : 논리주소 = 물리주소 (매우 비효율적, 과거에 사용)
    - Load Time Binding : 실행시 물리주소 결정
    - Execution Time Binding (Runtime Binding) : 실앻시 물리주소 결정되나 실행 도중 바뀔 수 있음
      - 여기서 MMU 사용 (Memory Management Unit)
      - Location Resister & Limit Resister

##### 페이지폴트

- 가상 메모리에는 존재하나 실제 메모르 (RAM) 에는 현재 없는 데이터 나 코드에 접근시 발생
  page fault routine 처리

1. 잘못된 요청(invalid reference) 인지 확인 : bad address, protection violation > ABORT
2. 아니라면 Get an empty page frame (없으면 뺏어옴)
3. 해당 페이지를 disk 에서 memory 로 읽어온다

- disc I/O 끝나기까지 이 프로세스는 CPU를 preempt당함 (block)
- disc read가 끝나면 page table entry 기록, bit invalid > valid
- ready queue에 process insert

4. 이 프로세스가 CPU를 잡고 다시 running (page fault 발생하지 않게 됨)
5. 아까 중단되었던 instruction 재개

##### 스와핑

- 페이지 폴트 발생시 메모리에서 당장 사용하지 않는 영역을 HDD 로 옮기고 HDD의 일부분을 마치 메모리처럼 불러와 쓰는 것
- 프로세스를 일시적으로 메모리에서 backing store(= swap area, HDD) 로 쫒아내는 방법
- 마치 페이지 폴트가 일어나지 않는 것처럼 만듬
- Swap in / swap out
- Priority Based Algorithm
  <br>
- 자주 발생되면 CPU 성능 저하
- 프로세스의 용량이 매우 크기 때문에 Swap time의 대부분은 transfer time
  (보통 HDD 접근 시간의 대부분은 disc head를 읽는 time이 대부분이다)
  <br>
- 페이지 : 가상 메모리를 사용하는 최소 크기 단위
- 프레임 : 실제 메모리를 사용하는 최소 크기 단위
  <br>
- 스와핑 과정
  1. CPU는 물리 메모리를 확인하여 해당 페이지가 없으면 트랩 발생 > OS 알림
  2. OS는 CPU 동작을 잠시 멈춤
  3. OS는 페이지 테이블 확인하여 가상 메모리에 페이지가 존재하는지 확인,
  4. 없으면 프로세스를 중단하고 현재 물리 메모리에 비어있는 프레임이 있는지 찾음
  5. 물리 메모리에도 없다면 스와핑 발생
  6. 비어있는 프레임에 해당 페이지를 로드, 페이지 테이블을 최신화
  7. 중단되었던 CPU를 다시 시작

#### 스레싱

- 메모리 페이지 폴트율이 높은 것을 의미
- 컴퓨터의 심각한 성능 저하

- 발생 원인

  1. 메모리에 너무 많은 프로세스가 동시에 올라가서 스와핑이 많이 일어남
  2. 그만큼 페이지 폴트가 많이 발생되었다는 뜻으로 CPU 이용률이 낮아짐
  3. OS는 CPU가 한가한다고 판단하여 더 많은 프로세스를 메모리에 올림
  4. 악순환이 반복되며 쓰레싱 발생

- 해결방법
  1. 메모리를 증가
  2. HDD > SDD
  3. 작업세트
  4. PFF
  5. 페이지 크기를 증가 (요즘 trend는 Larger Page Size)
  - Page Size를 감소시키면?
    - 페이지 수 증가
    - 페이지 테이블 증가
    - Internal fragmentation
    - Disk transfer의 효율성 감소
    - 메모리 이용면에서는 효율적이나 locality 활용 측면에서는 좋지 않음

##### 작업 세트

- 프로세스의 과거 사용 이력인 **지역성**을 통해 결정된 페이지 집합을 만들어 미리 메모리에 로드하는 것
- 탐색 비용 줄이고 스와핑도 줄일 수 있음

##### PFF (Page Fault Frequency)

- 페이지 폴트 빈도 조절하여 상한선과 하한선을 설정
- 상한선 도달시 프레임 증가, 하한선 도달시 프레임 감소시킴

#### 메모리 할당

- 메모리는 일반적으로 두 영역으로 사용
  - **OS 상주 영역 (커널영역)** : 낮은 주소 영역, interrupt vector와 함께 사용
  - **사용자 프로세스 영역** : 높은 주소 영역

##### 1. 연속 할당 방식 (Contiguous allocation)

- ##### 1.1 고정 분할 방식

  - 미리 메모리를 나누어 관리
  - 내부 단편화 문제
    - 내부 단편화 : 메모리를 나눈 크기보다 프로그램이 작아서 낭비되는 공간이 많이 발생하는 현상
    - 내부 조각 (Internal Fragmentation) : 하나의 분할 내부에서 발생하는 사용되지 않은 메모리 조각

- ##### 1.2 가변 분할 방식
- 매 시점 플그램의 크기에 맞게 동적으로 메모리를 나누어 사용
- 내부 단편화 해결되나 외부 단편화 발생 가능
  - 외부 단편화 : 메모리를 나눈 크기보다 프로그램이 커서 들어가지 못하는 현상
  - 외부 조각 (External Fragmentation) : 프로그램 크기보다 분할의 크기가 작은 경우
- 종류
  1. 최초적합 : 위쪽이나 아래쪽에서 탐색하여 홀 (빈 메모리 공간) 을 찾으면 바로 할당
  2. 최적접합 : 프로세스 크기 이상인 공간 중 가장 작은 홀부터 할당 (전체 리스트 탐색 필요)
  3. 최악 적합 : 프로세스의 크기와 가장 크기 차이가 많이 나는 홀부터 할당 (전체 리스트 탐색 필요)
- Compaction : 사용중인 메모리 영역을 한군데로 몰아 hole을 큰 block으로 만드는 방법 (외부조각 문제 해결)
  - 이는 큰 비용이 들며 최소한의 메모리 이동으로 compaction하기는 매우 복잡한 문제로 프로세스의 주소가 실행시간에 동적으로 재배치 가능한 경우에만 수행 가능

#### 2. 불연속 할당 (Noncontiguous allocation)

- 현대 OS가 쓰는 방법

- ### 2.1 페이징

  - 동일한 크기의 페이지 단위 (보통 4kb) 로 나누어 메모리의 서로 다른 위치에 프로세스 할당
  - 프로그램(프로세스)마다 페이지테이블을 두어 이를 통해 메모리에 프로세스 할당
  - 일부는 backing store에, 일부는 물리 메모리에 저장
  - virtual memory의 내용이 page 단위로 **불연속적**으로 저장됨
  - 홀의 크기가 균일하지 않은 문제 해결
  - 주소 변환 복잡해짐
  - #### Page Table
    : **_논리 page No > 물리 frame No 정보_**를 저장하는 배열, 프로그램 마다 별도로 존재 필요
    - 용량이 크기 때문에 **Main memory**에 상주
    - 모든 메모리 접근 연산에는 **2번의 메모리 접근 필요** (page table 1회 + data/instruction 1회)
      <br>
  - #### TLB : associative resistor or transition look-aside buffer

    - **속도 향상**을 위해 주소변환을 위한 **캐시 메모리**로 페이지 테이블에서 빈번히 참조되는 page No. & frame No. 일부를 저장하는 **HW**
    - **메모리 접근 1회만**
    - page table과의 차이 : p, f 쌍의 정보를 가지고 있어야 하며 // TLB에 정보가 있는지 확인하기 위해 **전체 항목에 대해 검색해야함 >> 병렬검색이 가능한 associative resistor로 구성**)
    - TLB는 context switch 때 flush (프로세스마다 별개 정보)

  - #### Two-Level Page Table

    - : Page Table 자체의 크기를 줄이기 위함

    - **TLB를 사용해도 여전히 page table은 Main memory에 존재**

    - page table 자체를 page로 구성현대의 컴퓨터의 논리주소공간은 32 bit-machine (4GB) (64 bit도 존재) >> page size가 4K시 **1M개의 page table entry 필요** >> page size가 4B시 **4M개의 page table entry 필요**

    - **그러나 대부분의 프로그램은 4G의 주소공간중 지극히 일부만 사용하기 때문에(code, data, stack 이외 빈 공간 많으나 page table entry는 빈 공간에 대해서도 만들어야함) page table 공간이 심하게 낭비**

    - **만약 offset을 늘려서 Page의 크기를 키우고 Page 개수를 줄여서 page table 공간을 줄인다면?**

      - **Internal Fragmentation 심화\***

    - **공간, 시간 모두 손해를 보는데 왜 2단계 페이지 테이블을 사용하는가?**

      - single page table 생성시 배열이라는 자료구조의 특성상 프로세스당 무조건 생성해야하는 절대적인 공간이 존재하나 multi-level page table의 경우 바깥테이블에서 사용 안되는 page는 안쪽 테이블에서 null로 공간 save

    - #### Multilevel Paging and Performance

      - level이 늘어날 수록 page table을 위한 공간이 줄어들지만 메모리 접근 시간이 늘어남
        **>> but TLB를 통해 메모리 접근 시간을 줄일 수 있음**
        _memory 접근 100nm, TLB 접근 20 nm, TLB hit ratio가 98%인 경우_
        _0.98 _ 120 + 0.02 _ 520 = 128 ns_
        _결과적으로 주소변환을 위해서는 28ns만 소요됨_

      * **장점**
        - 필요한 page table에 대해서만 메모리 할당
        - 메모리 관리가 linear page에 비해 쉬움 (4MB 연속 주소 필요 없이 page directory를 통해서 물리메모리의 빈공간에 page table entry 생성 가능)
      * **단점**
        - TLB miss & memory translation 1 > 2 증가
        - OS, HW에서 TLB miss Logic 및 메모리 성능 최적화 과정 복잡해짐

- ##### 2.2 세그멘테이션

  - 단순 물리 크기의 페이지 단위가 아닌 의미 단위인 세그먼트로 나누는 방식
  - 프로세스의 의미 단위

    - 코드, 데이터, 스택, 힙
    - 함수 단위 등

  - Segment Table

    - base (physical address starting point) + limit (length of segment > 균일하지 않기 때문 > 주소가 limit을 넘으면 trap)

  - **paging과의 차이**
    - page No가 아닌 base를 정확한 byte 단위 주소로 지정해줘야함
    - segment 길이가 일정하지 않기 때문에 Limit 값 지정 필요

  * 장점
    - **의미 단위 작업시 유리, paging 방식으로 할 시에 protection이 필요한 코드가 page 별로 섞여있을 수 있음**
    * 보안 (Protection) : ex) Protection bit 부여
    * 공유 (Shairing)
  * 단점
    - 홀 크기 불균형 문제 : Allocation시 External fragmentation (segment 길이가 동일하지 않기 때문에 가변분할 방식에서의 문제 발생 가능)

- ##### 2.3 페이지드 세그멘테이션
  - 공유나 보안을 의미 단위의 세그먼트로 나누고 && 물리적 메모리는 페이지로 나누는 것
  - 장점 : Allocation Problem 해결 (외부조각)
  * 단점 : 메모리 참조 횟수가 총 3회로 속도 저하 문제점 발생 가능성

#### 페이지 교체 알고리즘

- 주 목적 : 스와핑을 많이 일어나지 않도록

##### 1. 오프라인 알고리즘

- 먼 미래에 참조되는 페이지와 현재 할당하는 페이지를 바꾸는 알고리즘
- 가장 이상적인 알고리즘
- 그러나 미래의 사용될 프로세스 알 수 없음으로 사용은 불가능하나 다른 알고리즘과의 성능비교로 사용됨

##### 2. FIFO

- 가장 먼저 온 페이지를 교체 영역에 가장 먼저 놓는 것

* FIFO Anomaly : 메모리 크기를 늘려주면 오히려 성능이 나빠지는 특징

##### 3. LRU (Least Recently Use)

- 참조가 가장 오래된 페이지를 교체
- 이를 판단하기 위해 각 페이지마다 계수기, 스택을 두어야 하는 단점
- 보통 구현 시 2가지로 구성 (이중연결 리스트, 해시테이블)
  1. 이중 연결 리스트로 삽입 삭제 용이
  2. 해시테이블로 이중 연결리스트 탐색

##### 4. NUR (Not Used Recently) == Clock Algorithm

- Reference bit : **참조** 여부에 따라 0과 1을 가진 비트를 둠
  - 0 : 참조되지 않음
  - 1 : 최근에 참조 됨

* 성능 개선 by Modified bit (dirty bit) : 해당 페이지가 최근 **변경**되었을 시 HW가 1로 세팅
  - 메모리에서 쫓아낼 때 dirty bit이 0이면 그냥 교체, 1이면 변경된 내용을 backing store에 수정 후 교체
  - modified bit이 1인 페이지를 최대한 덜 쫓아내면 성능 개선 가능

- 시계방향으로 돌면서 0을 찾고 0을 찾은 순간 해당 프로세스로 교체, 해당 부분을 1로 바꾸는 알고리즘

* 구현 : Circular-LinkedList

##### 5. LFU (Least Frequently Used)

- 가장 참조 횟수가 적은 페이지를 교체

* 구현 : Heap으로 이진트리로 크기검색,수정 용이하게 O(log n) (직계가족과만 비교)

##### Paging System에서 LRU, LFU 가능한가?

만약 프로세스가 주소변환을 시도했을 때 이미 해당 페이지가 물리 메모리에 올라가있다면 (valid 라면) **OS는 하는일이 없음 (정보 전달이 안됨)**
**그러면 OS는 가장 마지막에/자주 참조된 페이지를 알 수 있는가? > NO!**
_>> Buffer or Web caching에서는 활용될 수 있음_

**※참고) Transparent 하다라는 의미는 무엇인가?**
(**https://kldp.org/node/93298** 댓글 참고 )
virtual : 없는데 있는 것 같이 보이는 것.
transparent : 있는데 없는 것 같이 보이는 것
